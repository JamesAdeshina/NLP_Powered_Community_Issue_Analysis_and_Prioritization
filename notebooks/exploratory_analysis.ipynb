{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting Up the Environment",
   "id": "629ef4c65adadaf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T22:39:40.037711Z",
     "start_time": "2025-02-26T22:39:39.779117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ],
   "id": "e4ca1cb3a144cdd0",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T22:39:43.977226Z",
     "start_time": "2025-02-26T22:39:42.680800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "id": "944fd11bc1e7916f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:38:45.867384Z",
     "start_time": "2025-02-25T23:38:45.824326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(\"Error loading data:\", e)\n",
    "        return None"
   ],
   "id": "8977d8c758a5d972",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:30:31.037176Z",
     "start_time": "2025-02-25T23:30:31.018386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_data(df):\n",
    "    \"\"\"\n",
    "    Analyze the data by printing summaries and distributions.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Data Shape:\", df.shape)\n",
    "    print(\"\\nFirst five rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\nSeverity distribution:\")\n",
    "    print(df['severity'].value_counts())\n",
    "\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts())"
   ],
   "id": "804af2e18378aa7c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:30:31.080439Z",
     "start_time": "2025-02-25T23:30:31.063946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_data(df):\n",
    "    \"\"\"\n",
    "    Visualize key distributions in the data.\n",
    "\n",
    "    \"\"\"\n",
    "    # Distribution of issue categories\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(x='issue_category', data=df, palette=\"viridis\")\n",
    "    plt.title(\"Issue Category Distribution\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Distribution of sentiment\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='sentiment', data=df, palette=\"magma\")\n",
    "    plt.title(\"Sentiment Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "712c02e0c10462d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:30:31.273594Z",
     "start_time": "2025-02-25T23:30:31.236452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from the text.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand contractions, e.g., don't -> do not.\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from the text.\"\"\"\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "def remove_mentions_hashtags(text):\n",
    "    \"\"\"Remove social media mentions and hashtags.\"\"\"\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation characters from the text.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \"\"\"Remove digits from the text.\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    \"\"\"\n",
    "    Normalize words with excessive repeated characters.\n",
    "    E.g., 'soooo' becomes 'so'.\n",
    "    \"\"\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"Remove extra whitespace and trim the text.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def tokenize_and_lower(text):\n",
    "    \"\"\"Tokenize the text and convert tokens to lowercase.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from the list of tokens.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatize tokens to their base form.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text using lemmatization.\n",
    "\n",
    "    Steps:\n",
    "    - Remove emojis.\n",
    "    - Expand contractions.\n",
    "    - Remove punctuation, URLs, mentions, hashtags, and numbers.\n",
    "    - Normalize repeated characters.\n",
    "    - Tokenize and lowercase.\n",
    "    - Remove stopwords.\n",
    "    - Lemmatize tokens.\n",
    "    \"\"\"\n",
    "    text = remove_emojis(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions_hashtags(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = normalize_repeated_chars(text)\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def remove_duplicates(df, column='text'):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows based on a specific column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        column (str): The column on which to base duplicate removal.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicate rows removed.\n",
    "    \"\"\"\n",
    "    before = df.shape[0]\n",
    "    df = df.drop_duplicates(subset=[column]).reset_index(drop=True)\n",
    "    after = df.shape[0]\n",
    "    print(f\"Removed {before - after} duplicate rows.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def comprehensive_process_data(df):\n",
    "    \"\"\"\n",
    "    Process and clean the data with comprehensive text preprocessing.\n",
    "\n",
    "    This function:\n",
    "    - Removes duplicate rows based on the 'text' column.\n",
    "    - Applies full text cleaning and preprocessing using both lemmatization and stemming.\n",
    "    - Processes other columns (e.g., issue_category, severity, sentiment) as needed.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Remove duplicate rows based on the 'text' column\n",
    "    df = remove_duplicates(df, column='text')\n",
    "\n",
    "    # Apply comprehensive text preprocessing\n",
    "    df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "    df['text_stemmed'] = df['text'].apply(preprocess_text_stemming)\n",
    "\n",
    "    # Process additional columns as needed\n",
    "    df['issue_category'] = df['issue_category'].astype(str).str.strip()\n",
    "    df['severity'] = df['severity'].fillna('Unknown')\n",
    "    df['sentiment'] = df['sentiment'].fillna('Neutral')\n",
    "\n",
    "    print(\"Comprehensive text processing complete.\")\n",
    "    return df\n"
   ],
   "id": "b2db818c44a7e2b2",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:39:32.585277Z",
     "start_time": "2025-02-25T23:39:32.186328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"../Data/processed/community_issues_dataset_template.csv\"\n",
    "\n",
    "\n",
    "load_data(file_path)\n"
   ],
   "id": "ff4be5af80e610aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                   text  \\\n",
       "0     Dear Sir/Madam, I am writing to express my dee...   \n",
       "1     Dear Council Team, I am writing to report the ...   \n",
       "2     To Whom It May Concern, I wish to bring to you...   \n",
       "3     Dear Sir/Madam, I am writing to highlight the ...   \n",
       "4     Dear Council Representative, I am compelled to...   \n",
       "...                                                 ...   \n",
       "1422  Dear Council, I’m writing to express my concer...   \n",
       "1423  Dear Council, I’m writing to share my positive...   \n",
       "1424  Dear Council, I’m writing to express my concer...   \n",
       "1425  Dear Council, I’m writing to share my neutral ...   \n",
       "1426  Dear Council, I’m writing to express my positi...   \n",
       "\n",
       "                  issue_category severity sentiment  \n",
       "0                  Air Pollution     High  Negative  \n",
       "1             Traffic Congestion   Medium  Negative  \n",
       "2                       Potholes     High  Negative  \n",
       "3                Noise Pollution      Low  Negative  \n",
       "4     Lack of Affordable Housing     High  Negative  \n",
       "...                          ...      ...       ...  \n",
       "1422  Crime Rates in Urban Areas   Medium   Neutral  \n",
       "1423  Crime Rates in Urban Areas      Low  Positive  \n",
       "1424  Crime Rates in Urban Areas     High  Negative  \n",
       "1425  Crime Rates in Urban Areas   Medium   Neutral  \n",
       "1426  Crime Rates in Urban Areas      Low  Positive  \n",
       "\n",
       "[1427 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>issue_category</th>\n",
       "      <th>severity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear Sir/Madam, I am writing to express my dee...</td>\n",
       "      <td>Air Pollution</td>\n",
       "      <td>High</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear Council Team, I am writing to report the ...</td>\n",
       "      <td>Traffic Congestion</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Whom It May Concern, I wish to bring to you...</td>\n",
       "      <td>Potholes</td>\n",
       "      <td>High</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir/Madam, I am writing to highlight the ...</td>\n",
       "      <td>Noise Pollution</td>\n",
       "      <td>Low</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Council Representative, I am compelled to...</td>\n",
       "      <td>Lack of Affordable Housing</td>\n",
       "      <td>High</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>Dear Council, I’m writing to express my concer...</td>\n",
       "      <td>Crime Rates in Urban Areas</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>Dear Council, I’m writing to share my positive...</td>\n",
       "      <td>Crime Rates in Urban Areas</td>\n",
       "      <td>Low</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>Dear Council, I’m writing to express my concer...</td>\n",
       "      <td>Crime Rates in Urban Areas</td>\n",
       "      <td>High</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>Dear Council, I’m writing to share my neutral ...</td>\n",
       "      <td>Crime Rates in Urban Areas</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>Dear Council, I’m writing to express my positi...</td>\n",
       "      <td>Crime Rates in Urban Areas</td>\n",
       "      <td>Low</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1427 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:42:49.200696Z",
     "start_time": "2025-02-25T23:42:48.933115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert CSV to DataFrame using load_data()\n",
    "df = load_data(file_path)\n",
    "\n",
    "analyze_data(df)"
   ],
   "id": "bde5df6a7930480f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Data Shape: (1427, 4)\n",
      "\n",
      "First five rows:\n",
      "                                                text  \\\n",
      "0  Dear Sir/Madam, I am writing to express my dee...   \n",
      "1  Dear Council Team, I am writing to report the ...   \n",
      "2  To Whom It May Concern, I wish to bring to you...   \n",
      "3  Dear Sir/Madam, I am writing to highlight the ...   \n",
      "4  Dear Council Representative, I am compelled to...   \n",
      "\n",
      "               issue_category severity sentiment  \n",
      "0               Air Pollution     High  Negative  \n",
      "1          Traffic Congestion   Medium  Negative  \n",
      "2                    Potholes     High  Negative  \n",
      "3             Noise Pollution      Low  Negative  \n",
      "4  Lack of Affordable Housing     High  Negative  \n",
      "\n",
      "Severity distribution:\n",
      "severity\n",
      "High        1147\n",
      "Medium       157\n",
      "Low          121\n",
      "severity       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "Negative     1150\n",
      "Positive      152\n",
      "Neutral       123\n",
      "sentiment       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:30:31.468863Z",
     "start_time": "2025-02-25T23:30:31.449757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Example usage within your main pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Update the file path to your data CSV file\n",
    "    file_path = \"Data/processed/community_issues_dataset_template.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading data:\", e)\n",
    "        df = None\n",
    "\n",
    "    if df is not None:\n",
    "        df = comprehensive_process_data(df)\n",
    "        print(df.head())"
   ],
   "id": "7ee875a538dd2bfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data: [Errno 2] No such file or directory: 'Data/processed/community_issues_dataset_template.csv'\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unsupervised Classification",
   "id": "30206dc646ba772d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def unsupervised_classification(texts, num_clusters=2):\n",
    "    \"\"\"\n",
    "    Convert texts to TF-IDF vectors and cluster them using KMeans.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): The list of letter texts.\n",
    "        num_clusters (int): Number of clusters to form.\n",
    "\n",
    "    Returns:\n",
    "        labels (list of int): Cluster labels for each text.\n",
    "        vectorizer (TfidfVectorizer): Fitted vectorizer.\n",
    "        kmeans (KMeans): Fitted KMeans model.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    return kmeans.labels_, vectorizer, kmeans\n",
    "\n",
    "\n",
    "\n",
    "def label_clusters(vectorizer, kmeans, local_keywords=['problem', 'issue', 'concern', 'maintenance', 'litter', 'noise']):\n",
    "    \"\"\"\n",
    "    Inspect cluster centroids to assign each cluster a label.\n",
    "    If a cluster's top terms include any local-problem keywords, label it as \"Local Problem\"; otherwise, \"New Initiatives\".\n",
    "\n",
    "    Args:\n",
    "        vectorizer (TfidfVectorizer): The TF-IDF vectorizer.\n",
    "        kmeans (KMeans): The fitted KMeans model.\n",
    "        local_keywords (list): Keywords to flag a cluster as local problems.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from cluster index to category label.\n",
    "    \"\"\"\n",
    "    cluster_labels = {}\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "        # If any local keyword is among the top terms, label as \"Local Problem\"\n",
    "        if any(keyword in top_terms for keyword in local_keywords):\n",
    "            cluster_labels[i] = \"Local Problem\"\n",
    "        else:\n",
    "            cluster_labels[i] = \"New Initiatives\"\n",
    "    return cluster_labels\n"
   ],
   "id": "c1349c5f360efbcb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
