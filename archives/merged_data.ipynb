{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:01.656028Z",
     "start_time": "2025-02-21T23:34:01.624114Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:01.732258Z",
     "start_time": "2025-02-21T23:34:01.728358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the folder where your CSV files are located\n",
    "folder_path = 'Data/raw'  # Replace with the actual path\n"
   ],
   "id": "1b233c1a3b85dadb",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:01.759292Z",
     "start_time": "2025-02-21T23:34:01.753219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List all the CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n"
   ],
   "id": "bf676e4fce8b79fe",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:01.849630Z",
     "start_time": "2025-02-21T23:34:01.835256Z"
    }
   },
   "cell_type": "code",
   "source": "print(csv_files)",
   "id": "4aaa589aeac92ee0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['community_issues_letters.csv', 'uk_issues_positive_sentiment.csv', 'community_issues_dataset_final.csv', 'community_issues_extended_dataset.csv', 'community_issues_dataset(1).csv', 'community_issues_dataset_updated.csv', 'community_issues_dataset_long.csv', 'uk_issues_negative_sentiment.csv', 'copilot.csv', 'community_issues_dataset-2.csv', 'community_issues_dataset copy.csv', 'community_issues_further_extended_dataset.csv', 'uk_issues_full_dataset.csv', 'community_issues_dataset.csv', 'community_issues_dataset_long copy.csv']\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:02.011859Z",
     "start_time": "2025-02-21T23:34:01.958343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize an empty list to store each CSV data as DataFrames\n",
    "dataframes = []"
   ],
   "id": "94b7f3a6e7bd679d",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:02.055113Z",
     "start_time": "2025-02-21T23:34:02.038661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize variables for column consistency check\n",
    "all_columns_match = True\n",
    "expected_columns = None  # This will store the expected columns if the first file is correct\n"
   ],
   "id": "acc2f3314a56f6f1",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:03.318560Z",
     "start_time": "2025-02-21T23:34:02.110184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Loop through each CSV file and read it into a DataFrame\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Check the column names of each file to debug\n",
    "    print(f\"Checking columns in {file}: {df.columns.tolist()}\")\n",
    "\n",
    "    # Check if the required 'Text' or 'Letter Text' column exists and standardize it to 'letter_text'\n",
    "    if 'Text' in df.columns:\n",
    "        df.rename(columns={'Text': 'letter_text'}, inplace=True)\n",
    "    elif 'Letter Text' in df.columns:\n",
    "        df.rename(columns={'Letter Text': 'letter_text'}, inplace=True)\n",
    "    else:\n",
    "        print(f\"Warning: Neither 'Text' nor 'Letter Text' column is present in {file}. Skipping this file.\")\n",
    "        continue  # Skip this file if neither column is found\n",
    "\n",
    "    # Check if the number of columns and their names match\n",
    "    if expected_columns is None:\n",
    "        # Set the columns of the first file as the expected structure\n",
    "        expected_columns = df.columns\n",
    "    elif list(df.columns) != list(expected_columns):\n",
    "        print(f\"Warning: Column mismatch in {file}. Skipping this file.\")\n",
    "        all_columns_match = False\n",
    "        continue  # Skip this file if columns do not match\n",
    "\n",
    "    # If checks pass, append the DataFrame to the list\n",
    "    dataframes.append(df)\n"
   ],
   "id": "7f73ffbdb079a6c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking columns in community_issues_letters.csv: ['Letter_ID', 'issue_category', 'Category', 'Severity', 'Frequency', 'sentiment', 'letter_text', 'Document Style']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_letters.csv. Skipping this file.\n",
      "Checking columns in uk_issues_positive_sentiment.csv: ['Letter ID', 'Severity', 'Frequency', 'Category', 'Sentiment', 'Issue Name', 'Content']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in uk_issues_positive_sentiment.csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset_final.csv: ['letter_text', 'issue_category', 'sentiment']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_dataset_final.csv. Skipping this file.\n",
      "Checking columns in community_issues_extended_dataset.csv: ['letter_text', 'issue_category', 'sentiment']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_extended_dataset.csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset(1).csv: ['letter_text', 'issue_category', 'sentiment']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_dataset(1).csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset_updated.csv: ['letter_text', 'issue_category', 'sentiment']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_dataset_updated.csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset_long.csv: ['Issue Name', 'Category', 'Severity', 'Frequency', 'Sentiment', 'Letter Text']\n",
      "Checking columns in uk_issues_negative_sentiment.csv: ['Letter ID', 'Severity', 'Frequency', 'Category', 'Sentiment', 'Issue Name', 'Content']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in uk_issues_negative_sentiment.csv. Skipping this file.\n",
      "Checking columns in copilot.csv: ['Letter ID', 'Severity', 'Frequency', 'issue_category', 'sentiment', 'issue_name', 'letter_text']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in copilot.csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset-2.csv: ['letter_text', 'issue_category', 'sentiment']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_dataset-2.csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset copy.csv: ['Issue Name', 'issue_category', 'Severity', 'Frequency', 'sentiment', 'letter_text']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_dataset copy.csv. Skipping this file.\n",
      "Checking columns in community_issues_further_extended_dataset.csv: ['letter_text', 'issue_category', 'sentiment']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_further_extended_dataset.csv. Skipping this file.\n",
      "Checking columns in uk_issues_full_dataset.csv: ['Letter ID', 'Severity', 'Frequency', 'Category', 'Sentiment', 'Issue Name', 'Content']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in uk_issues_full_dataset.csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset.csv: ['letter_text', 'issue_category', 'sentiment']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_dataset.csv. Skipping this file.\n",
      "Checking columns in community_issues_dataset_long copy.csv: ['issue_category', 'category', 'Severity', 'Frequency', 'sentiment', 'letter_text']\n",
      "Warning: Neither 'Text' nor 'Letter Text' column is present in community_issues_dataset_long copy.csv. Skipping this file.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:03.341048Z",
     "start_time": "2025-02-21T23:34:03.331382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# If columns mismatch, notify the user\n",
    "if not all_columns_match:\n",
    "    print(\"Not all files have matching columns or required 'Text'/'Letter Text' column. Skipped incompatible files.\")\n"
   ],
   "id": "3da64258afb6ea28",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:03.442722Z",
     "start_time": "2025-02-21T23:34:03.418638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Concatenate all DataFrames into one single DataFrame\n",
    "if dataframes:\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Print out the columns of the merged dataset for debugging\n",
    "    print(\"Merged Dataset Columns:\", merged_df.columns.tolist())\n",
    "\n",
    "    # Check if the necessary columns are present before selecting them\n",
    "    columns_to_select = ['letter_text']\n",
    "\n",
    "    # If 'issue_category' exists, add it to the selected columns\n",
    "    if 'issue_category' in merged_df.columns:\n",
    "        columns_to_select.append('issue_category')\n",
    "\n",
    "    # If 'sentiment' exists, add it to the selected columns\n",
    "    if 'sentiment' in merged_df.columns:\n",
    "        columns_to_select.append('sentiment')\n",
    "\n",
    "    # Select only the columns you need\n",
    "    merged_df = merged_df[columns_to_select]\n",
    "\n",
    "    # Check the first few rows of the merged dataset\n",
    "    print(\"Merged Dataset:\")\n",
    "    print(merged_df.head())\n",
    "else:\n",
    "    print(\"No valid files to merge.\")"
   ],
   "id": "94ebf5e774df94ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset Columns: ['Issue Name', 'Category', 'Severity', 'Frequency', 'Sentiment', 'letter_text']\n",
      "Merged Dataset:\n",
      "                                         letter_text\n",
      "0  Dear Council,\\n\\nI am writing to formally rais...\n",
      "1  Dear Council,\\n\\nI am writing to formally rais...\n",
      "2  Dear Council,\\n\\nI am writing to formally rais...\n",
      "3  Dear Council,\\n\\nI am writing to formally rais...\n",
      "4  Dear Council,\\n\\nI am writing to formally rais...\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:03.692922Z",
     "start_time": "2025-02-21T23:34:03.622508Z"
    }
   },
   "cell_type": "code",
   "source": "merged_df.describe()",
   "id": "e456a7eb460cde58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              letter_text\n",
       "count                                               97000\n",
       "unique                                                 97\n",
       "top     Dear Council,\\n\\nI am writing to formally rais...\n",
       "freq                                                 1314"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Dear Council,\\n\\nI am writing to formally rais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T23:34:08.263202Z",
     "start_time": "2025-02-21T23:34:03.950430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Optionally: Save the merged dataset to a new CSV file\n",
    "# merged_df.to_csv('merged_data.csv', index=False)"
   ],
   "id": "1e0db790989aac5",
   "outputs": [],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
